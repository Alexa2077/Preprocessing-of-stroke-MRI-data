# 1 概述
**Note1：请识别归一化和标准化，注意这两种方法的异同，以及使用场景。**  

## 1.1 概述
数据的归一化和标准化是**特征缩放(feature scaling)**的方法，是数据预处理的关键步骤。不同评价指标往往具有不同的**量纲**和量纲单位，这样的情况会影响到数据分析的结果，为了消除指标之间的量纲影响，需要进行数据归一化/标准化处理，以解决数据指标之间的可比性。原始数据经过数据归一化/标准化处理后，各指标处于同一数量级，适合进行综合对比评价。  
归一化/标准化实质是一种**线性变换**，线性变换有很多良好的性质，这些性质决定了对数据改变后不会造成“失效”，反而能提高数据的表现，这些性质是归一化/标准化的前提。比如有一个很重要的性质：线性变换不会改变原始数据的数值排序。具体作用可总结如下：  
（1）**某些模型求解需要**  
在使用梯度下降的方法求解最优化问题时， 归一化/标准化后**可以加快梯度下降**的求解速度，即提升模型的收敛速度。  
一些分类器需要计算样本之间的距离(如欧氏距离)，例如KNN。如果一个特征值域范围非常大，那么距离计算就主要取决于这个特征，从而与实际情况相悖(比如这时实际情况是值域范围小的特征更重要)。  

（2）**无量纲化**  
	例如房子数量和收入，因为从业务层知道，这两者的重要性一样，所以把它们全部归一化。 这是从业务层面上作的处理。  
（3）**避免数值问题**  
	太大的数会引发数值问题。  

## 1.1 意义详细解释：  
比如说，z-score 能够真实的反应一个分数距离平均数的相对标准距离。如果我们把每一个分数都转换成z分数，那么每一个z分数会以标准差为单位表示一个具体分数到平均数的距离或离差。将成正态分布的数据中的原始分数转换为z分数，我们就可以通过查阅z分数在正态曲线下面积的表格来得知平均数与z分数之间的面积，进而得知原始分数在数据集合中的百分等级。一个数列的各z分数的平方和等于该数列数据的个数，并且z分数的标准差和方差都为1.平均数为0.  
听不懂？ 再简单点说吧 ！Z-Score的主要目的就是将**不同量级的数据统一转化为同一个量级**，**统一用计算出的Z-Score值衡量**，**以保证数据之间的可比性**。z-score值只是一个临界值，它是标准化的结果，**本身没有意义，只能用于比较。**注意，一般来说**z-score不是归一化，而是标准化，归一化只是标准化的一种。**  
听完以后还是感觉懵懵哒？再具体来说吧！在使用深度学习方法，如CNN对图像做分类时，会经常对图像做标准化预处理操作。  
首先我们来看这样一张图：  
![image](https://github.com/Alexa2077/Preprocessing-of-stroke-MRI-data/assets/59952693/26a956b8-1c21-4079-a098-dfd654fa2631)

大家看上面两只佩奇，对于人来说，它就是两只一样的猪，只是图片的灰度或者曝光度不一样罢了，于是我们都给它们都标注为社会人，咋一看，好像没毛病。但是，虽然我们人眼看没毛病，可是机器看的方式和我们不一样，他们看的是对应图片的像素值。由于曝光的，灰度等各种原因，他们像素值其实不一样，那么经过卷积层后，他们的特征很可能不一样。于是神经网络就尴尬了，特征都不一样，为啥标签都一样呢？**这样，迷惑的神经网络就不知道怎么对权值进行训练了。**  

**所以，在对图像进行标准化之前：**  
对于人来说，同一样的事物，一样的标签。  
但是对于机器来说，像素值大小不同，可能是两张完全不同的图片，最后经过卷积层，又可能是不同的特征，但却是同样的标签，进而影响神经网络的权值更新。  

**标准化以后：**  
两张图片的像素值就被映射成符合正太分布，均值为0，标准差为1的数值分布：  
这就相当于把不同的图片映射到同一个坐标系，具有相同的尺度；  
因此上述情况就由像素值大小不同的问题转为具有相似的特征分布的问题；  
一定程度上消除了因为过度曝光，质量不佳或者噪声各种原因对模型权重值更新的影像。  


**还有一种情况：**  
![image](https://github.com/Alexa2077/Preprocessing-of-stroke-MRI-data/assets/59952693/0b0a0a7d-88df-411d-ac3b-e884222c8f39)

![image](https://github.com/Alexa2077/Preprocessing-of-stroke-MRI-data/assets/59952693/0132aa3a-a940-455e-8b7b-a0819bc9cb54)

![image](https://github.com/Alexa2077/Preprocessing-of-stroke-MRI-data/assets/59952693/9374f8d8-cc17-4306-8bd7-7bc1371bb895)

![image](https://github.com/Alexa2077/Preprocessing-of-stroke-MRI-data/assets/59952693/0c709c5c-edff-4e84-b182-3b82bb496fbc)


标准化之后：    
图像中的RGB分量就被映射成符合正太分布，均值为0，标准差为1的数值分布：  
相当于把不同的分量映射到同一个坐标系。具有相同的尺度；  
上述情况就由过大像素值分量主导权值更新的问题转为RGB分量都具有相同的数值分布的问题；  
一定程度上消除了梯度更新的收敛慢，无序等问题 。  
  

当然，上面仅举了两个例子，还有很多情况需要进行标准化才能解决，标准化常用的是Z-Score, 记住，这是要减去自己数据的均值和除以自己的标准差（不是方差）！  


# 2，标准化 Standardization
## 2.1 Z-score标准化方法
**Z-score standardization：**Z-Score标准化是数据处理的一种常用方法。通过它能够将不同量级的数据转化为**统一量度**的Z-Score分值进行比较。**提高了数据可比性，削弱了数据解释性**。  
定义：这种方法给与原始数据的均值（mean）和标准差（standard deviation）进行数据的标准化。均值是0，标准差为1。  
本质：把有量纲表达式变成无量纲表达式。  
**转换函数：（X-Mean）/(Standard deviation)**  
其中，Mean为所有样本数据的均值。Standard deviation为所有样本数据的标准差。  
**作用：**  
提升模型的收敛速度（加快梯度下降的求解速度）  
提升模型的精度（消除量级和量纲的影响）  
简化计算（与归一化的简化原理相同）  


# 3，归一化 Normalization  
归一化一般是将数据映射到指定的范围，用于去除不同维度数据的量纲以及量纲单位。  
常见的映射范围有 [0, 1] 和 [-1, 1] ，最常见的归一化方法就是 Min-Max 归一化。  
**Min-Max 归一化（Min-Max Normalization）**  
也称为**离差标准化**，是对原始数据的线性变换，**使结果值映射到[0 - 1]之间**。转换函数如下：  
![image](https://github.com/Alexa2077/Preprocessing-of-stroke-MRI-data/assets/59952693/f11ce005-283f-44f2-b5b3-efbb7cba7d4d)

其中max为样本数据的最大值，min为样本数据的最小值。这种归一化方法比较适用在数值比较集中的情况。但是，如果max和min不稳定，很容易使得归一化结果不稳定，使得后续使用效果也不稳定，实际使用中可以用经验常量值来替代max和min。而且当有新数据加入时，可能导致max和min的变化，需要重新定义
**有时候我们希望将输入转换到[-1,1]的范围，可以使用以下的公式：**  
![image](https://github.com/Alexa2077/Preprocessing-of-stroke-MRI-data/assets/59952693/46d38892-428b-4508-8f82-d4d7c0c3ccbe)

以上两种方式，都是针对原始数据做等比例的缩放。其中 Xnorm 是归一化以后的数据， X 是原始数据大小， Xmax,Xmin 分别是原始数据的最大值与最小值。  
**作用：**  
数据映射到指定的范围内进行处理，更加便捷快速。  
把有量纲表达式变成无量纲表达式，便于不同单位或量级的指标能够进行比较和加权。经过归一化后，将有量纲的数据集变成纯量，还可以达到简化计算的作用。  


# 4 归一化和标准化的异同  
（1）**区别**  
1，Normalization会**严格的限定变换后数据的范围**，比如最大最小值处理的Normalization将样本的特征值转换到同一量纲下把数据映射到[0,1]或者[-1, 1]区间内。而**Standardization就没有严格的区间**，变换后的数据没有范围，只是其均值是0，标准差为1。  
2，归一化(Normalization)对数据的缩**放比例仅仅和极值有关**，就是说比如100个数，你除去极大值和极小值其他数据都更换掉，缩放比例![image](https://github.com/Alexa2077/Preprocessing-of-stroke-MRI-data/assets/59952693/71140343-fcd7-47a5-a527-4d46c91988f2)
是不变的；反观，对于标准化(Standardization)而言，标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，转换为标准正态分布，和**整体样本分布相关**，每个样本点都能对标准化产生影响。  

（2）**相同**
	它们的相同点在于都能**取消由于量纲不同引起的误差**；都是一种线性变换，都是对向量X按照比例压缩再进行平移。  

# 5 归一化和标准化的适用场景  
（1）数据的分布本身就服从**正态分布**，使用Z-Score。  
（2）有**离群值**的情况：使用Z-Score。这里不是说有离群值时使用Z-Score不受影响，而是，**Min-Max**对于离群值十分敏感，因为离群值的出现，会影响数据中max或min值，从而使Min-Max的效果很差。相比之下，虽然使用Z-Score计算方差和均值的时候仍然会受到离群值的影响，但是相比于Min-Max法，影响会小一点。  
（3）如果对输出结果**范围有要求**，用**归一化**。  
（4）如果数据较为稳定，**不存在极端的最大最小值，用归一化**。  
（5）如果数据存在异常值和较多噪音，**用标准化**，可以间接通过中心化避免异常值和极端值的影响。  
（6）**在分类、聚类算法中，需要使用距离来度量相似性的时候**、或者使用PCA技术进行降维的时候，标准化表现更好；**在不涉及距离度量、协方差计算的时候，可以使用归一化方法**。  
PS：PCA中标准化表现更好的原因可以参考  

**所有情况都应当Standardization或Normalization么？**  
	当原始数据不同维度特征的尺度(量纲)不一致时，需要标准化步骤对数据进行标准化或归一化处理，反之则不需要进行数据标准化。也不是所有的模型都需要做归一的，比如模型算法里面有没关于对距离的衡量，没有关于对变量间标准差的衡量。**比如决策树**，他采用算法里面没有涉及到任何和距离等有关的，所以在做决策树模型时，通常是不需要将变量做标准化的；另外，**概率模型不需要归一化**，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率。  

# 6 不同类型医学图像进行的标准化与归一化处理  
# 6.1 CT 
如果是CT，可以首先统计一下金标准mask所覆盖的前景目标的灰度值的以下特征：  
1.均值  
2.方差  
3.去最小0.5%极端值后的最小值（简称0.05%最小值）  
4.去最大0.5%极端值后的最大值（简称99.5%最大值）  
然后采用以下规则进行标准化：  
1.clip掉0.5%最小值以下的值和99.5%最大值以上的值：  
2.减均值除以方差  

# 6.2 MRI
mri的话，因为磁场和各种参数的原因，同一个组织在不同次扫描里面灰度值并不太固定，所以不建议采用以上方法。建议直接最大最小值(也可以用去0.5%的极值)归一化到正负1或0,1之间。  


  

# 7 参考：
[https://zhuanlan.zhihu.com/p/296252799](https://zhuanlan.zhihu.com/p/296252799):概要内容参考  
[https://blog.csdn.net/weixin_36604953/article/details/102652160](https://blog.csdn.net/weixin_36604953/article/details/102652160)  
[https://www.cnblogs.com/Renyi-Fan/p/13842738.html](https://www.cnblogs.com/Renyi-Fan/p/13842738.html)  

不同类型医学图像进行的标准化与归一化处理参考：  
请问深度学习医学图像分割，如何归一化？ - 张良怀的回答 - 知乎 https://www.zhihu.com/question/379900540/answer/1411664196 
